apiVersion: kubeflow-mini.io/v1
kind: MLJob
metadata:
  name: mnist-training
  namespace: default
spec:
  type: pytorch
  frameworkVersion: "1.13.1"
  workerReplicas: 1
  image: pytorch/pytorch:1.13.1-cuda11.6-cudnn8-runtime
  command:
    - "python"
    - "-c"
    - |
      import torch
      import torch.nn as nn
      import torch.optim as optim
      from torchvision import datasets, transforms
      
      # 定义简单的CNN模型
      class Net(nn.Module):
          def __init__(self):
              super(Net, self).__init__()
              self.conv1 = nn.Conv2d(1, 32, 3, 1)
              self.conv2 = nn.Conv2d(32, 64, 3, 1)
              self.dropout1 = nn.Dropout(0.25)
              self.dropout2 = nn.Dropout(0.5)
              self.fc1 = nn.Linear(9216, 128)
              self.fc2 = nn.Linear(128, 10)
      
          def forward(self, x):
              x = self.conv1(x)
              x = nn.functional.relu(x)
              x = self.conv2(x)
              x = nn.functional.relu(x)
              x = nn.functional.max_pool2d(x, 2)
              x = self.dropout1(x)
              x = torch.flatten(x, 1)
              x = self.fc1(x)
              x = nn.functional.relu(x)
              x = self.dropout2(x)
              x = self.fc2(x)
              return nn.functional.log_softmax(x, dim=1)
      
      # 设置训练参数
      batch_size = 64
      epochs = 5
      device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
      
      # 加载MNIST数据集
      transform = transforms.Compose([
          transforms.ToTensor(),
          transforms.Normalize((0.1307,), (0.3081,))
      ])
      dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
      train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)
      
      # 初始化模型和优化器
      model = Net().to(device)
      optimizer = optim.Adadelta(model.parameters())
      
      # 训练模型
      model.train()
      for epoch in range(epochs):
          for batch_idx, (data, target) in enumerate(train_loader):
              data, target = data.to(device), target.to(device)
              optimizer.zero_grad()
              output = model(data)
              loss = nn.functional.nll_loss(output, target)
              loss.backward()
              optimizer.step()
              if batch_idx % 100 == 0:
                  print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '
                        f'({100. * batch_idx / len(train_loader):.0f}%)]\tLoss: {loss.item():.6f}') 